{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FpnzzBZtedrB"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"elahezohdi\"\n",
        "os.environ['KAGGLE_KEY'] = \"b4d81db0a12258d41bbdde98d1131209\"\n",
        "\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews\n",
        "!unzip -q amazon-books-reviews.zip -d data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdmTHQV-effW",
        "outputId": "cc0a0d9e-203f-44ea-b935-1e5549024214"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "Downloading amazon-books-reviews.zip to /content\n",
            " 96% 1.02G/1.06G [00:04<00:00, 297MB/s]\n",
            "100% 1.06G/1.06G [00:04<00:00, 230MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.options(header=True, inferSchema=True).csv(\"data/Books_rating.csv\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOgStfYXf3Co",
        "outputId": "81f1214a-e119-4990-a851-25fbbb37cede"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "|1882931173|Its Only Art If I...| null| AVCGYZL8FQQTD|\"Jim of Oz \"\"jim-...|               7/7|         4.0|  940636800|Nice collection o...|This is only for ...|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A30TK6U7DNS82R|       Kevin Killian|             10/10|         5.0| 1095724800|   Really Enjoyed It|I don't care much...|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A3UH4UZ4RSVO82|        John Granger|             10/11|         5.0| 1078790400|Essential for eve...|\"If people become...|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A2MVUWT453QH61|\"Roy E. Perry \"\"a...|               7/7|         4.0| 1090713600|Phlip Nel gives s...|Theodore Seuss Ge...|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A22X4XUPKF66MR|\"D. H. Richards \"...|               3/3|         4.0| 1107993600|Good academic ove...|\"Philip Nel - Dr....|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A2F6NONFUDB6UK|              Malvin|               2/2|         4.0| 1127174400|One of America's ...|\"\"\"Dr. Seuss: Ame...|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A14OJS0VWMOSWO| Midwest Book Review|               3/4|         5.0| 1100131200|A memorably excel...|Theodor Seuss Gie...|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A2RSSXTDZDUSH4|           J. Squire|               0/0|         5.0| 1231200000|Academia At It's ...|\"When I recieved ...|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A25MD5I2GUIW6W|\"J. P. HIGBED \"\"b...|               0/0|         5.0| 1209859200|And to think that...|\"Trams (or any pu...|\n",
            "|0826414346|Dr. Seuss: Americ...| null|A3VA4XFS5WNJO3|     Donald Burnside|               3/5|         4.0| 1076371200|Fascinating accou...|As far as I am aw...|\n",
            "|0829814000|Wonderful Worship...|19.40| AZ0IOBU20TBOP|  Rev. Pamela Tinnin|              8/10|         5.0|  991440000|Outstanding Resou...|I just finished t...|\n",
            "|0829814000|Wonderful Worship...|19.40|A373VVEU6Z9M0N|Dr. Terry W. Dorsett|               1/1|         5.0| 1291766400|Small Churches CA...|\"Many small churc...|\n",
            "|0829814000|Wonderful Worship...|19.40| AGKGOH65VTRR4|\"Cynthia L. Lajoy...|               1/1|         5.0| 1248307200|Not Just for Past...|I just finished r...|\n",
            "|0829814000|Wonderful Worship...|19.40| A3OQWLU31BU1Y|       Maxwell Grant|               1/1|         5.0| 1222560000|Small church past...|\"I hadn't been a ...|\n",
            "|0595344550|Whispers of the W...|10.95|A3Q12RK71N74LB|         Book Reader|              7/11|         1.0| 1117065600|            not good|I bought this boo...|\n",
            "|0595344550|Whispers of the W...|10.95|A1E9M6APK30ZAU|           V. Powell|               1/2|         4.0| 1119571200|  Here is my opinion|\"I have to admit,...|\n",
            "|0595344550|Whispers of the W...|10.95| AUR0VA5H0C66C|\"LoveToRead \"\"Act...|               1/2|         1.0| 1119225600|        Buyer beware|\"This is a self-p...|\n",
            "|0595344550|Whispers of the W...|10.95|A1YLDZ3VHR6QPZ|               Clara|               2/4|         5.0| 1115942400| Fall on your knee's|When I first read...|\n",
            "|0595344550|Whispers of the W...|10.95| ACO23CG8K8T77|               Tonya|               5/9|         5.0| 1117065600|      Bravo Veronica|I read the review...|\n",
            "|0595344550|Whispers of the W...|10.95|A1VK81CRRC7MLM|\"missyLou \"\"apple\"\"\"|               1/3|         5.0| 1130025600|           Wonderful|\"I really enjoyed...|\n",
            "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print schema: column names and data types\n",
        "df.printSchema()\n",
        "\n",
        "# Show a few rows to understand the content\n",
        "df.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm5ZbfvwgdN5",
        "outputId": "45e3cf56-256c-40be-a776-2459ed2dcdbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Title: string (nullable = true)\n",
            " |-- Price: string (nullable = true)\n",
            " |-- User_id: string (nullable = true)\n",
            " |-- profileName: string (nullable = true)\n",
            " |-- review/helpfulness: string (nullable = true)\n",
            " |-- review/score: string (nullable = true)\n",
            " |-- review/time: string (nullable = true)\n",
            " |-- review/summary: string (nullable = true)\n",
            " |-- review/text: string (nullable = true)\n",
            "\n",
            "+----------+------------------------------+-----+--------------+--------------------------------------+------------------+------------+-----------+-----------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Id        |Title                         |Price|User_id       |profileName                           |review/helpfulness|review/score|review/time|review/summary                                 |review/text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
            "+----------+------------------------------+-----+--------------+--------------------------------------+------------------+------------+-----------+-----------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1882931173|Its Only Art If Its Well Hung!|null |AVCGYZL8FQQTD |\"Jim of Oz \"\"jim-of-oz\"\"\"             |7/7               |4.0         |940636800  |Nice collection of Julie Strain images         |This is only for Julie Strain fans. It's a collection of her photos -- about 80 pages worth with a nice section of paintings by Olivia.If you're looking for heavy literary content, this isn't the place to find it -- there's only about 2 pages with text and everything else is photos.Bottom line: if you only want one book, the Six Foot One ... is probably a better choice, however, if you like Julie like I like Julie, you won't go wrong on this one either.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "|0826414346|Dr. Seuss: American Icon      |null |A30TK6U7DNS82R|Kevin Killian                         |10/10             |5.0         |1095724800 |Really Enjoyed It                              |I don't care much for Dr. Seuss but after reading Philip Nel's book I changed my mind--that's a good testimonial to the power of Rel's writing and thinking. Rel plays Dr. Seuss the ultimate compliment of treating him as a serious poet as well as one of the 20th century's most interesting visual artists, and after reading his book I decided that a trip to the Mandeville Collections of the library at University of California in San Diego was in order, so I could visit some of the incredible Seuss/Geisel holdings they have there.There's almost too much to take in, for, like William Butler Yeats, Seuss led a career that constantly shifted and metamoprhized itself to meet new historical and political cirsumstances, so he seems to have been both a leftist and a conservative at different junctures of his career, both in politics and in art. As Nel shows us, he was once a cartoonist for the fabled PM magazine and, like Andy Warhol, he served his time slaving in the ad business too. All was in the service of amusing and broadening the minds of US children. Nel doesn't hesitate to administer a sound spanking to the Seuss industry that, since his death, has seen fit to license all kinds of awful products including the recent CAT IN THE HAT film with Mike Myers. Oh, what a cat-astrophe!The book is great and I can especially recommend the work of the picture editor who has given us a bounty of good illustrations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "|0826414346|Dr. Seuss: American Icon      |null |A3UH4UZ4RSVO82|John Granger                          |10/11             |5.0         |1078790400 |Essential for every personal and Public Library|\"If people become the books they read and if \"\"the child is father to the man                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
            "|0826414346|Dr. Seuss: American Icon      |null |A2MVUWT453QH61|\"Roy E. Perry \"\"amateur philosopher\"\"\"|7/7               |4.0         |1090713600 |Phlip Nel gives silly Seuss a serious treatment|Theodore Seuss Geisel (1904-1991), aka &quot;Dr. Seuss,&quot; was one of the most influential writers and artists of the 20th century.In 1959, Rudolf Flesch wrote, &quot;A hundred years from now, children and their parents will still eagerly read the books of a fellow called Ted Geisel, popularly known as Dr. Seuss.&quot;Flesch was too conservative in his prediction. A century, and more, from today, Dr. Seuss will still be read when many authors on today's bestseller lists will be forgotten.Published on the centenary of Geisel's birth, Dr. Seuss: American Icon analyzes six key aspects of Seuss's career: poetry, politics, art, biography, marketing, and influence.In six insightful chapters, Philip Nel, Assistant Professor of English at Kansas State University, discusses &quot;U.S. Laureate of Nonsense,&quot; &quot;Dr. Seuss vs. Adolf Hitler,&quot; &quot;The Doc in the Smock,&quot; &quot;The 5,000 Fingers of Dr. S.,&quot; &quot;The Disneyfication of Dr. Seuss,&quot; and &quot;The Cat in the Hat for President.&quot;Nel gives short shrift to Geisel's childhood and family background--and, indeed, to biography in general--preferring to focus on Seuss's writing and art, from his first book, And to Think That I Saw It on Mulberry Street (1937) to his last book, Oh, the Places You'll Go! (1990).Dr. Seuss's breakthrough year was 1957, when he published the two books with which he is most often identified: The Cat in the Hat and How the Grinch Stole Christmas!Other classic works in the Seussian canon are: Horton Hears a Who! (&quot;A person's a person, no matter how small&quot;), Yertle the Turtle (modeled on the rise of Adolf Hitler), Green Eggs and Ham (Seuss's bestselling book), The Sneeches (a criticism of anti-Semitism), The Lorax (a protest against corporate abuse of the environment), and The Butter Battle Book (a critique of Reagan's enthusiasm for the nuclear arms trace).His favorite work, among the books he authored, was The Cat in the Hat, for it, more than any other, taught children to read.While many of his books have a clear and powerful moral, Seuss had a horror of heavy-handed preaching. He sought to teach and ignite the imagination, but was a lifelong opponent of smug, self-righteous bourgeois moralism.&quot;Seuss was a contrarian,&quot; writes Nel, &quot;who enjoyed challenging people to reconsider their assumptions. [He had a] rebellious imagination and a dispositional distaste for rules and regulations.&quot; His work was a &quot;rational insanity&quot; that exhibited &quot;joyous anarchy&quot; and a &quot;lifelong thrill in misbehaving.&quot;A better subtitle for Nel's work would have been American Icon and Iconoclast.Nel tells of Seuss's early years as an advertising artist and as a agitprop cartoonist. The book, however, is not a biography; it is a serious study in the genres of literary and art criticism.For readers who want more biographical information, Nel recommends Dr. Seuss and Mr. Geisel, by Judith Morgan and Neil Morgan (1995), which he describes as &quot;the definitive biography and the single best secondary source on Seuss. Any discussion of Seuss's life and work must begin with this book.&quot;Dr. Seuss: American Icon includes 103 pages of notes, index, and the most comprehensive annotated Seuss bibliography ever assembled. One learns a lot from this book; the author's lucid style makes it not only an enlightening work but a fun read.Philip Nel is the author of J. K. Rowling's Harry Potter Novels: A Reader's Guide (2001) and The Avant-Garde and American Postmodernity (2002).Roy E. Perry of Nolensville, Tennessee, is an advertising copywriter at a Nashville publishing house.|\n",
            "|0826414346|Dr. Seuss: American Icon      |null |A22X4XUPKF66MR|\"D. H. Richards \"\"ninthwavestore\"\"\"   |3/3               |4.0         |1107993600 |Good academic overview                         |\"Philip Nel - Dr. Seuss: American IconThis is basically an academic overview of Seuss poetry, art, cartoons, and the problems with the commercialization of the Seuss name and works after his death. It is not, to any real extent, a biography. Those seeking such should move on.As an academic book it leans on the dry side. It assumes the reader has a fairly good knowledge of Children's Literature and 20th Century cartoons (not the animated kind). Not a book to begin your Dr. Seuss experience with. But if you have read them to your children and are interested about the writing style (there is a good chapter about his poetry) or his art style (not as good a chapter, but still interesting).What interested me the most was the deconstruction of the recent rush to \"\"cash in\"\" on Seuss by Hollywood and advertisers. I think that Nel wants to come down against it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
            "+----------+------------------------------+-----+--------------+--------------------------------------+------------------+------------+-----------+-----------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select(\"User_id\", \"Title\")"
      ],
      "metadata": {
        "id": "GWK17IT8giEX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "# Count nulls per column\n",
        "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpvklihihfhx",
        "outputId": "453d6da7-0dfe-42e8-9b3a-6ed805d40f55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|User_id|Title|\n",
            "+-------+-----+\n",
            "| 562250|  208|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values in either column\n",
        "df = df.dropna(subset=[\"User_id\", \"Title\"])\n",
        "\n",
        "# Drop exact duplicate reviews\n",
        "df = df.dropDuplicates([\"User_id\", \"Title\"])\n"
      ],
      "metadata": {
        "id": "aeEu_Jj2hh5l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Unique users:\", df.select(\"User_id\").distinct().count())\n",
        "print(\"Unique titles:\", df.select(\"Title\").distinct().count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsEa3hLThuIk",
        "outputId": "460029af-dd5f-4cd3-a494-8a91654cecc4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique users: 1008423\n",
            "Unique titles: 206640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show examples of similar book titles\n",
        "from pyspark.sql.functions import lower\n",
        "\n",
        "df_lower = df.withColumn(\"Title\", lower(col(\"Title\")))"
      ],
      "metadata": {
        "id": "RKpayTkahwSg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "WlYuIWMTi8hF",
        "outputId": "56a63639-acd5-4188-f2b9-e394f9797818"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-------+-------------------+--------------------+\n",
              "|summary|            User_id|               Title|\n",
              "+-------+-------------------+--------------------+\n",
              "|  count|            2114559|             2114559|\n",
              "|   mean|              23.35|   2071.738021978022|\n",
              "| stddev|  33.92977175153278|  2403.9374661206234|\n",
              "|    min| \"\" Film acting \"\"\"|  \"\"\" Film technique|\n",
              "|    max|      AZZZZW74AAX75|xBase Programming...|\n",
              "+-------+-------------------+--------------------+"
            ],
            "text/html": [
              "<table border='1'>\n",
              "<tr><th>summary</th><th>User_id</th><th>Title</th></tr>\n",
              "<tr><td>count</td><td>2114559</td><td>2114559</td></tr>\n",
              "<tr><td>mean</td><td>23.35</td><td>2071.738021978022</td></tr>\n",
              "<tr><td>stddev</td><td>33.92977175153278</td><td>2403.9374661206234</td></tr>\n",
              "<tr><td>min</td><td> &quot;&quot; Film acting &quot;&quot;&quot;</td><td>&quot;&quot;&quot; Film technique</td></tr>\n",
              "<tr><td>max</td><td>AZZZZW74AAX75</td><td>xBase Programming...</td></tr>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df.sample(False, 0.01, seed=42)\n",
        "print(\"Sample size:\", df_sample.count())\n",
        "df_sample.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fr8rcPJi-IG",
        "outputId": "85c075c5-b5de-45e5-e7d9-41b63905068b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size: 21280\n",
            "+--------------+---------------------------------------------------------------------------------+\n",
            "|User_id       |Title                                                                            |\n",
            "+--------------+---------------------------------------------------------------------------------+\n",
            "|ARD5I31J12UU8 |The Mayor of Casterbridge (Signet classics)                                      |\n",
            "|A3N8RZSXKN1OR2|Sword and sorceress xiii                                                         |\n",
            "|A3IUPJ2SSLG7RH|The Complete Book of Chinese Health Balls: Background and Use of the Health Balls|\n",
            "|A1VQEAFW83OTG9|Rabbit, run                                                                      |\n",
            "|AJHPZ48YOZK4H |Zen and the Art of Motorcycle Maintenance                                        |\n",
            "+--------------+---------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import collect_set\n",
        "\n",
        "# Group titles into baskets per user\n",
        "df_baskets = df.groupBy(\"User_id\").agg(collect_set(\"Title\").alias(\"Basket\"))\n",
        "\n",
        "# Show a few examples of user baskets\n",
        "df_baskets.show(5, truncate=False)\n",
        "\n",
        "# Count total number of baskets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XujjIIKEjAMG",
        "outputId": "36eba7c1-6b3c-4a35-f1c6-951778c71cb2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------------------------------------------------------------------+\n",
            "|User_id              |Basket                                                                         |\n",
            "+---------------------+-------------------------------------------------------------------------------+\n",
            "|A06765203K4HQ4HP9CW9A|[The Power of the Actor]                                                       |\n",
            "|A1005YJDO9VCIY       |[Choose Peace & Happiness: A 52-Week Guide, The Gift of the Acorn]             |\n",
            "|A1006V961PBMKA       |[DAVE GROHL, Fighting The Forces: What's At Stake In Buffy The Vampire Slayer?]|\n",
            "|A100DWG2GUBPF2       |[Purple America.]                                                              |\n",
            "|A100HWDN5JMK8G       |[Jake's Gold]                                                                  |\n",
            "+---------------------+-------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 1% of the baskets for faster testing (disable later for full run)\n",
        "df_sampled_baskets = df_baskets.sample(fraction=0.01, seed=42)\n",
        "\n",
        "# Check how many baskets are in the sample\n",
        "print(\"Sample size:\", df_sampled_baskets.count())\n",
        "\n",
        "# Show a few sample baskets\n",
        "df_sampled_baskets.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MjhV0N-jBwu",
        "outputId": "e5fb7f1b-29e3-4382-82d0-39d5fd10934f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size: 10191\n",
            "+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|User_id       |Basket                                                                                                                                            |\n",
            "+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|A168DC570805X |[Cosmic Voyage: True Evidence of Extraterrestrials Visiting Earth]                                                                                |\n",
            "|A17MHHEGHJ7GMO|[The Night Before Christmas, The (Wee Books for Wee Folk), A visit from Santa Claus, Twas the night before Christmas;: A visit from St. Nicholas,]|\n",
            "|A19W94F9XQKV0L|[A Biblical Defense Guide for Gays, Lesbians and Those Who Love Them]                                                                             |\n",
            "|A1IEX07KC6YZM8|[The Black Stallion]                                                                                                                              |\n",
            "|A1UVAP2PM7DBYW|[Italian English Bilingual Visual Dictionary (DK Visual Dictionaries)]                                                                            |\n",
            "+--------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.fpm import FPGrowth\n",
        "\n",
        "# Retry with lower minSupport\n",
        "fpGrowth = FPGrowth(itemsCol=\"Basket\", minSupport=0.001, minConfidence=0.1)\n",
        "model = fpGrowth.fit(df_sampled_baskets)\n",
        "\n",
        "# Get frequent itemsets and rules\n",
        "freq_itemsets = model.freqItemsets\n",
        "assoc_rules = model.associationRules\n",
        "\n",
        "print(\"Frequent Itemsets:\")\n",
        "freq_itemsets.show(10, truncate=False)\n",
        "\n",
        "print(\"Association Rules:\")\n",
        "assoc_rules.show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EA4mlmEjDq2",
        "outputId": "5df6493c-1092-4328-fb3c-5643c3e86fed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "+------------------------------------------------------------------------------------------------------------------+----+\n",
            "|items                                                                                                             |freq|\n",
            "+------------------------------------------------------------------------------------------------------------------+----+\n",
            "|[The Hobbit There and Back Again]                                                                                 |40  |\n",
            "|[The Hobbit]                                                                                                      |40  |\n",
            "|[The Hobbit, The Hobbit There and Back Again]                                                                     |40  |\n",
            "|[The Hobbit or There and Back Again]                                                                              |40  |\n",
            "|[The Hobbit or There and Back Again, The Hobbit]                                                                  |40  |\n",
            "|[The Hobbit or There and Back Again, The Hobbit, The Hobbit There and Back Again]                                 |40  |\n",
            "|[The Hobbit or There and Back Again, The Hobbit There and Back Again]                                             |40  |\n",
            "|[The Hobbitt, or there and back again; illustrated by the author.]                                                |40  |\n",
            "|[The Hobbitt, or there and back again; illustrated by the author., The Hobbit or There and Back Again]            |40  |\n",
            "|[The Hobbitt, or there and back again; illustrated by the author., The Hobbit or There and Back Again, The Hobbit]|40  |\n",
            "+------------------------------------------------------------------------------------------------------------------+----+\n",
            "only showing top 10 rows\n",
            "\n",
            "Association Rules:\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------+----------+-----------------+---------------------+\n",
            "|antecedent                                                                                                                         |consequent                                            |confidence|lift             |support              |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------+----------+-----------------+---------------------+\n",
            "|[Jane Eyre (Signet classics), Jane Eyre (Everyman's Classics), Jane Eyre (Penguin Classics), Jane Eyre, Jane Eyre (Simple English)]|[Jane Eyre (Large Print)]                             |1.0       |783.9230769230769|0.001275635364537337 |\n",
            "|[Jane Eyre (Signet classics), Jane Eyre (Everyman's Classics), Jane Eyre (Penguin Classics), Jane Eyre, Jane Eyre (Simple English)]|[Jane Eyre: Complete and Unabridged (Puffin Classics)]|1.0       |783.9230769230769|0.001275635364537337 |\n",
            "|[Jane Eyre (Signet classics), Jane Eyre (Everyman's Classics), Jane Eyre (Penguin Classics), Jane Eyre, Jane Eyre (Simple English)]|[Jane Eyre (New Windmill)]                            |1.0       |783.9230769230769|0.001275635364537337 |\n",
            "|[Jane Eyre (New Windmill), Jane Eyre (Signet classics), Jane Eyre (Large Print)]                                                   |[Jane Eyre]                                           |1.0       |783.9230769230769|0.001275635364537337 |\n",
            "|[Jane Eyre (New Windmill), Jane Eyre (Signet classics), Jane Eyre (Large Print)]                                                   |[Jane Eyre (Penguin Classics)]                        |1.0       |783.9230769230769|0.001275635364537337 |\n",
            "|[Jane Eyre (New Windmill), Jane Eyre (Signet classics), Jane Eyre (Large Print)]                                                   |[Jane Eyre (Simple English)]                          |1.0       |783.9230769230769|0.001275635364537337 |\n",
            "|[Jane Eyre (New Windmill), Jane Eyre (Signet classics), Jane Eyre (Large Print)]                                                   |[Jane Eyre (Everyman's Classics)]                     |1.0       |783.9230769230769|0.001275635364537337 |\n",
            "|[Jane Eyre (New Windmill), Jane Eyre (Signet classics), Jane Eyre (Large Print)]                                                   |[Jane Eyre: Complete and Unabridged (Puffin Classics)]|1.0       |783.9230769230769|0.001275635364537337 |\n",
            "|[Of Mice and Men (Penguin Audiobooks), Of Mice and Men Hb (New Windmill), Of Mice and Men]                                         |[OF MICE AND MEN]                                     |1.0       |849.2499999999999|0.0011775095672652341|\n",
            "|[Of Mice and Men (Penguin Audiobooks), Of Mice and Men Hb (New Windmill), Of Mice and Men]                                         |[Of Mice And Men]                                     |1.0       |849.2499999999999|0.0011775095672652341|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------+----------+-----------------+---------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df.dropna(subset=[\"User_id\", \"Title\"]).dropDuplicates([\"User_id\", \"Title\"])\n"
      ],
      "metadata": {
        "id": "fZnUYccQxs-O"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 1% of your cleaned DataFrame (adjust sample size as needed)\n",
        "df_sample = df_clean.sample(withReplacement=False, fraction=0.01, seed=42)"
      ],
      "metadata": {
        "id": "_3jBrGJ2jFO1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.show(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22nBq8LmjHDf",
        "outputId": "1eccdcad-0cbf-439e-926b-e6cf6a5f24c9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------------+\n",
            "|       User_id|               Title|\n",
            "+--------------+--------------------+\n",
            "| ARD5I31J12UU8|The Mayor of Cast...|\n",
            "|A3N8RZSXKN1OR2|Sword and sorcere...|\n",
            "|A3IUPJ2SSLG7RH|The Complete Book...|\n",
            "+--------------+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "# Convert baskets to a single comma-separated string\n",
        "baskets_df = df_baskets.withColumn(\"basket_str\", concat_ws(\", \", \"Basket\")).select(\"basket_str\")\n",
        "\n",
        "# Convert to RDD of strings\n",
        "rdd = baskets_df.rdd.map(lambda row: row['basket_str'])\n",
        "\n",
        "# Split into list of book titles\n",
        "baskets = rdd.map(lambda line: line.split(\", \"))\n",
        "\n",
        "# You can now work with baskets as RDD[list of strings]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6kUBNj9ajP00"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QuP0105Av3vo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Flatten function defined properly at the top-level\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "# 2. Hashing vocabulary: flat and distinct items\n",
        "flat_items_rdd = baskets.flatMap(identity).distinct().zipWithIndex()\n",
        "\n",
        "# 3. Safe collect (this is what Nicolaus did)\n",
        "hash_index = dict(flat_items_rdd.collect())\n",
        "\n",
        "# 4. Hashing the baskets\n",
        "def hashing(basket):\n",
        "    return {hash_index[item] for item in basket if item in hash_index}\n",
        "\n",
        "hashed_baskets = baskets.map(hashing)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "id": "-y2htGOnjUKC",
        "outputId": "1f13a2de-74ac-43a2-eb59-940061b4be58"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 437, in dumps\n",
            "    return cloudpickle.dumps(obj, pickle_protocol)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 72, in dumps\n",
            "    cp.dump(obj)\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 540, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 630, in reducer_override\n",
            "    return self._function_reduce(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 503, in _function_reduce\n",
            "    return self._dynamic_function_reduce(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 484, in _dynamic_function_reduce\n",
            "    state = _function_getstate(func)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 156, in _function_getstate\n",
            "    f_globals_ref = _extract_code_globals(func.__code__)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n",
            "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n",
            "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
            "                 ~~~~~^^^^^^^\n",
            "IndexError: tuple index out of range\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Could not serialize object: IndexError: tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mreducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mnewargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_getnewargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_function_getstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         return (types.FunctionType, newargs, state, None, None,\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mf_globals_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_code_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     f_globals = {k: func.__globals__[k] for k in f_globals_ref if k in\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3084100243>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 2. Hashing vocabulary: flat and distinct items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mflat_items_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaskets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzipWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 3. Safe collect (this is what Nicolaus did)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mdistinct\u001b[0;34m(self, numPartitions)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \"\"\"\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                    \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \"\"\"\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0mlocally_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombineLocally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m         \u001b[0mshuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocally_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_mergeCombiners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mpartitionBy\u001b[0;34m(self, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             pairRDD = self.ctx._jvm.PairwiseRDD(\n\u001b[0;32m-> 2081\u001b[0;31m                 keyed._jrdd.rdd()).asJavaPairRDD()\n\u001b[0m\u001b[1;32m   2082\u001b[0m             jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,\n\u001b[1;32m   2083\u001b[0m                                                            id(partitionFunc))\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2947\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2949\u001b[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0m\u001b[1;32m   2950\u001b[0m                                       self._jrdd_deserializer, profiler)\n\u001b[1;32m   2951\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2830\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2815\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "def a_priori(baskets_rdd, support_threshold):\n",
        "    print(\"=== A-Priori Algorithm ===\")\n",
        "\n",
        "    # Pass 1: Count singletons\n",
        "    singleton_counts = baskets_rdd.flatMap(lambda basket: [(item, 1) for item in basket]) \\\n",
        "                                  .reduceByKey(lambda a, b: a + b) \\\n",
        "                                  .filter(lambda x: x[1] >= support_threshold)\n",
        "\n",
        "    singletons = singleton_counts.map(lambda x: (x[0],)).collect()\n",
        "    print(\"Frequent singletons:\", len(singletons))\n",
        "\n",
        "    k = 2\n",
        "    current_frequents = set(singletons)\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\nGenerating itemsets of size {k}...\")\n",
        "        candidate_counts = baskets_rdd.flatMap(\n",
        "            lambda basket: [(tuple(sorted(c)), 1) for c in combinations(basket, k)\n",
        "                            if all(tuple(sorted(sub)) in current_frequents for sub in combinations(c, k-1))]\n",
        "        ).reduceByKey(lambda a, b: a + b).filter(lambda x: x[1] >= support_threshold)\n",
        "\n",
        "        count = candidate_counts.count()\n",
        "        if count == 0:\n",
        "            print(f\"No frequent itemsets of size {k}. Stopping.\")\n",
        "            break\n",
        "\n",
        "        current_frequents = set(candidate_counts.map(lambda x: x[0]).collect())\n",
        "        print(f\"Frequent itemsets of size {k}: {count}\")\n",
        "        k += 1\n"
      ],
      "metadata": {
        "id": "TCyPi_CDjVl5"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the support threshold to something meaningful\n",
        "support_threshold = 50  # try 20 or 10 for testing\n",
        "a_priori(baskets, support_threshold)\n"
      ],
      "metadata": {
        "id": "2IFxK0og4pZ5",
        "outputId": "ebe57984-0f61-4c92-979a-d59d41c74b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== A-Priori Algorithm ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\", line 437, in dumps\n",
            "    return cloudpickle.dumps(obj, pickle_protocol)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 72, in dumps\n",
            "    cp.dump(obj)\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 540, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 630, in reducer_override\n",
            "    return self._function_reduce(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 503, in _function_reduce\n",
            "    return self._dynamic_function_reduce(obj)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 484, in _dynamic_function_reduce\n",
            "    state = _function_getstate(func)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 156, in _function_getstate\n",
            "    f_globals_ref = _extract_code_globals(func.__code__)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n",
            "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n",
            "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
            "                 ~~~~~^^^^^^^\n",
            "IndexError: tuple index out of range\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Could not serialize object: IndexError: tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     71\u001b[0m             )\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mreducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamic_function_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mnewargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_getnewargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_function_getstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         return (types.FunctionType, newargs, state, None, None,\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mf_globals_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_code_globals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     f_globals = {k: func.__globals__[k] for k in f_globals_ref if k in\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mout_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moparg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moparg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_walk_global_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-2391659723>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Adjust the support threshold to something meaningful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msupport_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m  \u001b[0;31m# try 20 or 10 for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma_priori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaskets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-2079064772>\u001b[0m in \u001b[0;36ma_priori\u001b[0;34m(baskets_rdd, support_threshold)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Pass 1: Count singletons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msingleton_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaskets_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mbasket\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbasket\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                   \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                                   \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msupport_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         \"\"\"\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0mlocally_combined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombineLocally\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreservesPartitioning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m         \u001b[0mshuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocally_combined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_mergeCombiners\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mpartitionBy\u001b[0;34m(self, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             pairRDD = self.ctx._jvm.PairwiseRDD(\n\u001b[0;32m-> 2081\u001b[0;31m                 keyed._jrdd.rdd()).asJavaPairRDD()\n\u001b[0m\u001b[1;32m   2082\u001b[0m             jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,\n\u001b[1;32m   2083\u001b[0m                                                            id(partitionFunc))\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2947\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2949\u001b[0;31m         wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\n\u001b[0m\u001b[1;32m   2950\u001b[0m                                       self._jrdd_deserializer, profiler)\n\u001b[1;32m   2951\u001b[0m         python_rdd = self.ctx._jvm.PythonRDD(self._prev_jrdd.rdd(), wrapped_func,\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     return sc._jvm.PythonFunction(bytearray(pickled_command), env, includes, sc.pythonExec,\n\u001b[1;32m   2830\u001b[0m                                   sc.pythonVer, broadcast_vars, sc._javaAccumulator)\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2813\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2815\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0;31m# The broadcast will have same life cycle as created PythonRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.1.1-bin-hadoop3.2/python/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "coKNwPY643g2"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}